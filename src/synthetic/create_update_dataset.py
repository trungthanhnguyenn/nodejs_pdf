import os
from tqdm import tqdm
import csv
from datasets import load_dataset, Dataset, DatasetDict
import sys
import pandas as pd

system_prompt = r"""
B·∫°n l√† m·ªôt chuy√™n gia ph√°p lu·∫≠t c√≥ nhi·ªám v·ª• **tr√≠ch xu·∫•t th√¥ng tin c√≥ c·∫•u tr√∫c** t·ª´ vƒÉn b·∫£n ph√°p lu·∫≠t ƒë√£ ƒë∆∞·ª£c s·ªë h√≥a (OCR ho·∫∑c ƒë·ªãnh d·∫°ng vƒÉn b·∫£n th∆∞·ªùng).

Y√™u c·∫ßu b·∫Øt bu·ªôc:
- D·ª±a v√†o ph·∫ßn `context` b√™n d∆∞·ªõi, h√£y ƒëi·ªÅn d·ªØ li·ªáu v√†o **b·∫£ng JSON** ƒë√∫ng theo ƒë·ªãnh d·∫°ng v√† t√™n tr∆∞·ªùng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh.
- N·∫øu **thi·∫øu qu√° nhi·ªÅu tr∆∞·ªùng th√¥ng tin quan tr·ªçng**, h√£y tr·∫£ v·ªÅ chu·ªói duy nh·∫•t:
  `PDF kh√¥ng ch·ª©a ƒë·ªß th√¥ng tin ƒë·ªÉ ƒëi·ªÅn v√†o b·∫£ng.`
- Tr·∫£ v·ªÅ ƒë·ªëi t∆∞·ª£ng JSON **g·ªëc, kh√¥ng b·ªçc trong chu·ªói**.
- Tr∆∞·ªùng `"noi_dung"` ph·∫£i ch·ª©a **to√†n b·ªô n·ªôi dung vƒÉn b·∫£n t·ª´ ph·∫ßn **Qu·ªëc hi·ªáu Ti√™u ng·ªØ tr·ªü xu·ªëng**, kh√¥ng ƒë∆∞·ª£c r√∫t g·ªçn ho·∫∑c m√¥ t·∫£ b·∫±ng l·ªùi.
- Ch·ªâ tr·∫£ v·ªÅ ƒë√∫ng m·ªôt trong hai:
  1. M·ªôt ƒë·ªëi t∆∞·ª£ng JSON ƒë·∫ßy ƒë·ªß.
  2. Ho·∫∑c chu·ªói `"PDF kh√¥ng ch·ª©a ƒë·ªß th√¥ng tin ƒë·ªÉ ƒëi·ªÅn v√†o b·∫£ng."`

---

C·∫•u tr√∫c JSON y√™u c·∫ßu:
{
  "so_hieu": "...",                  // S·ªë hi·ªáu vƒÉn b·∫£n (VD: 01/2023/TT-BGDƒêT)
  "loai_vb": "...",                  // Lo·∫°i vƒÉn b·∫£n (VD: Th√¥ng t∆∞, Ngh·ªã ƒë·ªãnh)
  "noi_ban_hanh": "...",            // C∆° quan ban h√†nh (VD: B·ªô T√†i ch√≠nh)
  "nguoi_ky": "...",                // Ng∆∞·ªùi k√Ω ban h√†nh
  "ngay_ban_hanh": "...",           // Ng√†y ban h√†nh (DD/MM/YYYY)
  "ngay_hieu_luc": "...",           // Ng√†y c√≥ hi·ªáu l·ª±c (DD/MM/YYYY)
  "ngay_cong_bao": "...",           // Ng√†y c√¥ng b·ªë
  "so_cong_bao": "...",             // S·ªë C√¥ng b√°o
  "tinh_trang": "...",              // Tr·∫°ng th√°i hi·ªáu l·ª±c (VD: C√≤n hi·ªáu l·ª±c)
  "tieu_de": "...",                 // T√™n vƒÉn b·∫£n
  "noi_dung": "...",                // To√†n b·ªô n·ªôi dung c·ªßa vƒÉn b·∫£n, kh√¥ng t√≥m t·∫Øt
  "linh_vuc": "..."                 // Lƒ©nh v·ª±c (VD: Gi√°o d·ª•c, Y t·∫ø)
}
"""

human_prompt = "H√£y tr√≠ch xu·∫•t th√¥ng tin theo y√™u c·∫ßu."

class AutoCreateDataRequest():
    def __init__(self, txt_folder, output_csv_new, error_log_path, repo_id, urls_path):
        self.txt_folder = txt_folder
        self.output_csv_new = output_csv_new
        self.error_log_path = error_log_path
        self.repo_id = repo_id
        self.urls_path = urls_path
        # self.hf_csv = hf_csv
        
    def make_new_csv(self):
        # Load all titles from urls.txt
        with open(self.urls_path, mode="r", encoding="utf-8") as f:
            lines = f.readlines()
            titles = [line.split("|||")[0].strip() for line in lines]

        txt_files = [f for f in os.listdir(self.txt_folder) if f.endswith('.txt')]
        error_count = 0

        os.makedirs(os.path.dirname(self.error_log_path), exist_ok=True)

        ##### First, clean the txt folder:
        with open(self.error_log_path, "w", encoding="utf-8") as log_file:
            for filename in tqdm(txt_files, desc="Cleaning TXT files"):
                file_path = os.path.join(self.txt_folder, filename)
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read().strip()
                        if "ƒêang t·∫£i vƒÉn b·∫£n..." in content:
                            os.remove(file_path)
                            log_file.write(f"Removed empty file: {os.path.splitext(filename)[0]}\n")
                            error_count += 1
                except Exception as e:
                    log_file.write(f"Error processing {filename}: {str(e)}\n")
                    error_count += 1

        # Regenerate file list after cleanup
        txt_files = sorted([f for f in os.listdir(self.txt_folder) if f.endswith('.txt')])
                    
        os.makedirs(os.path.dirname(self.output_csv_new), exist_ok=True)

        ###### Then, create the CSV file:
        with open(self.output_csv_new, mode="w", newline='', encoding="utf-8") as csvfile:
            writer = csv.DictWriter(
                csvfile,
                fieldnames=["title", "system", "human", "context"],
                quoting=csv.QUOTE_ALL
            )
            writer.writeheader()

            for filename in tqdm(txt_files, desc="Creating CSV"):
                file_path = os.path.join(self.txt_folder, filename)
                try:
                    inx = int(os.path.splitext(filename)[0]) - 1
                    raw_title = titles[inx] if 0 <= inx < len(titles) else f"UNKNOWN_TITLE_{filename}"
                    title = raw_title.replace("/", "_")
                    
                    with open(file_path, "r", encoding="utf-8") as f:
                        raw_context = f.read().strip().replace('"', '""')  # escape d·∫•u "
                    #     title = os.path.splitext(filename)[0]

                        writer.writerow({
                            "title": title,
                            "system": system_prompt.strip(),
                            "human": human_prompt,
                            "context": raw_context
                        })
                except Exception as e:
                    print(f"Error processing {filename}: {str(e)}")
                    with open(self.error_log_path, "a", encoding="utf-8") as log_file:
                        log_file.write(f"Error processing {filename}: {str(e)}\n")
                    error_count += 1
        # dataset = Dataset.from_csv(self.output_csv)
        # data = DatasetDict({'train': dataset})
        # data.push_to_hub(repo_id=self.repo_id, max_shard_size='150MB')
                          
    def download_compare_update(self):
        """
        Download the latest dataset from Hugging Face and update the local CSV file.
        """
        dataset = load_dataset(self.repo_id, split='train')
        df = dataset.to_pandas()
        
        df_new = pd.read_csv(self.output_csv_new)
        df_merged = pd.concat([df, df_new], ignore_index=True)
        print(f"Total rows after merging: {len(df_merged)}")
        
        df_merged = df_merged.drop_duplicates(subset=['title'], keep='first')
        print(f"Total rows after removing duplicates: {len(df_merged)}")
        
        # dataset = Dataset.from_pandas(df_merged)
        # DatasetDict({"train": dataset}).push_to_hub(repo_id=self.repo_id, max_shard_size="150MB")
        
        # print(f"Upload new data to Hugging Face: {self.repo_id}")   
  
def check_csv(csv_file: str, index_to_check: int):
  csv.field_size_limit(sys.maxsize)  # ‚úÖ TƒÉng gi·ªõi h·∫°n cho √¥ d·ªØ li·ªáu l·ªõn

  with open(csv_file, mode="r", encoding="utf-8") as f:
      reader = csv.DictReader(f)
      rows = list(reader)

      if index_to_check < len(rows):
          row = rows[index_to_check]
          print(f"üîé D√≤ng s·ªë {index_to_check}:")
          print(f"üìå Title:\n{row['title']}\n")
          print(f"üìå System:\n{row['system']}\n")
          print(f"üìå Human:\n{row['human']}\n")
          print(f"üìå Context (1000 k√Ω t·ª± ƒë·∫ßu):\n{row['context'][:1000]}...\n")
      else:
          print(f"‚ùå Index {index_to_check} v∆∞·ª£t qu√° s·ªë d√≤ng trong CSV ({len(rows)})")

# check_csv("demo.csv", 301)
if __name__ == "__main__":
    # Create a new dataset
    data = AutoCreateDataRequest(
        txt_folder="output/txt",
        output_csv_new="output/csv/demo.csv",
        error_log_path="logs/error/txts_fail.log",
        repo_id="trungnguyen2331/law_extract",
        urls_path="urls.txt",
    )
    data.make_new_csv()
    data.download_compare_update()
